{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Assignment (4).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwrI5pnhrI_A"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJm1gqm1mY0N"
      },
      "source": [
        "# # install prerequisites for nle\n",
        "# %%capture\n",
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y build-essential autoconf libtool pkg-config \\\n",
        "#     python3-dev python3-pip python3-numpy git libncurses5-dev \\\n",
        "#     libzmq3-dev flex bison "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF6VyA00nG9-"
      },
      "source": [
        "# # download, build and install flatbuffers\n",
        "# %%capture\n",
        "# !git clone https://github.com/google/flatbuffers.git\n",
        "# # all these commands have to be run in the same directory and !cd doesn't change\n",
        "# # the directory permanently in colab see: \n",
        "# # https://stackoverflow.com/questions/48298146/changing-directory-in-google-colab-breaking-out-of-the-python-interpreter\n",
        "# !cd flatbuffers && cmake -G \"Unix Makefiles\" && make && sudo make install"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aumo0H2dmH-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5035f06a-7d4b-49f0-800d-50a148657364"
      },
      "source": [
        "# # %%capture\n",
        "# # the next step requires a version of cmake > 3.14.0\n",
        "# !pip3 install cmake==3.15.3\n",
        "# # add -v for verbose if there are any errors\n",
        "# !pip3 install nle \n",
        "# %cd /content/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cmake==3.15.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/34/0a311fedffcc7a153bbc0390ef4c378dbc7f09f9865247137f82d62f8e7a/cmake-3.15.3-py3-none-manylinux2010_x86_64.whl (16.5MB)\n",
            "\u001b[K     |████████████████████████████████| 16.5MB 1.4MB/s \n",
            "\u001b[?25hInstalling collected packages: cmake\n",
            "  Found existing installation: cmake 3.12.0\n",
            "    Uninstalling cmake-3.12.0:\n",
            "      Successfully uninstalled cmake-3.12.0\n",
            "Successfully installed cmake-3.15.3\n",
            "Collecting nle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/8d/04662ffda71aa358cc8ee0a04e70d6a769fe8140371ec006d8555a7a1e87/nle-0.6.0.tar.gz (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 3.7MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.15 in /usr/local/lib/python3.6/dist-packages (from nle) (0.17.3)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached https://files.pythonhosted.org/packages/00/84/fc9dc13ee536ba5e6b8fd10ce368fea5b738fe394c3b296cde7c9b144a92/pybind11-2.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from nle) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.15->nle) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.15->nle) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.15->nle) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.15->nle) (0.16.0)\n",
            "Building wheels for collected packages: nle\n",
            "  Building wheel for nle (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nle: filename=nle-0.6.0-cp36-cp36m-linux_x86_64.whl size=2847879 sha256=3f4419a5fe821f3882bad2a0db2c6a112923d90712cc5bb7442001257bad91f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/4d/7c/e4c74b776f945ec1bc9bf01dc94bc226e452cf7dd2aba347a2\n",
            "Successfully built nle\n",
            "Installing collected packages: pybind11, nle\n",
            "Successfully installed nle-0.6.0 pybind11-2.6.1\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpEtdEG7mOI8"
      },
      "source": [
        "import nle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PjvAgt7rOq4"
      },
      "source": [
        "# Agents\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug0UBgenrSrK"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Initialise a buffer of a given size for storing transitions\n",
        "        :param size: the maximum number of transitions that can be stored\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        data = (state, action, reward, next_state, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            data = self._storage[i]\n",
        "            state, action, reward, next_state, done = data\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(next_state)\n",
        "            dones.append(done)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(next_states),\n",
        "            np.array(dones),\n",
        "        )\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
        "        return self._encode_sample(indices)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmAkTdUBrrgR"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "        replay_buffer: ReplayBuffer,\n",
        "        use_double_dqn,\n",
        "        lr,\n",
        "        batch_size,\n",
        "        gamma,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the DQN algorithm using the Adam optimiser\n",
        "        :param action_space: the action space of the environment\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param replay_buffer: storage for experience replay\n",
        "        :param lr: the learning rate for Adam\n",
        "        :param batch_size: the batch size\n",
        "        :param gamma: the discount factor\n",
        "        \"\"\"\n",
        "        self.observation_space = observation_space\n",
        "        map_observation_space = self.observation_space[\"glyphs\"]\n",
        "        stats_observation_space = self.observation_space[\"blstats\"]\n",
        "        \n",
        "        self.action_space = action_space\n",
        "        self.n_actions = action_space.n\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size \n",
        "        #replay_memory_capacity batch to update/use at once NOT its capacity/size\n",
        "        self.gamma = gamma\n",
        "        self.replay_memory = replay_buffer\n",
        "        \n",
        "        #model = policy_network\n",
        "        self.model = DQN(map_observation_space, stats_observation_space, self.action_space).cuda()\n",
        "        self.use_double_dqn = use_double_dqn \n",
        "        self.target_model = DQN(map_observation_space, stats_observation_space, self.action_space).cuda()\n",
        "        #Copies parameters (weights and biases) and buffers from model\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        #DQN has parameters from nn.Module\n",
        "        #optimiser must be made AFTER moving movel to GPU\n",
        "        self.optimiser = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "        #RND stuff\n",
        "        \n",
        "        self.target_network = DQN(map_observation_space, stats_observation_space, self.action_space).cuda()\n",
        "        self.predictor_network = DQN(map_observation_space, stats_observation_space, self.action_space).cuda()\n",
        "        self.rnd_optimiser = torch.optim.Adam(self.predictor_network.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "\n",
        "        # if replay buffer not filled up to full size\n",
        "        #if len(self.replay_memory) < self.batch_size:\n",
        "        #    return None\n",
        "\n",
        "        #can also call states, actions,rewards... = \n",
        "        transitions = self.replay_memory.sample(self.batch_size)\n",
        "        \n",
        "        # because our states are dictionaries \n",
        "        batch_map_states = []\n",
        "        batch_stats_states = []\n",
        "        batch_next_map_states = []\n",
        "        batch_next_stats_states = []\n",
        "\n",
        "        for i in range(len(transitions[0])):\n",
        "            # transitions[0] is the state [3] is the next state\n",
        "            batch_map_states.append(transitions[0][i]['glyphs'])\n",
        "            batch_stats_states.append(transitions[0][i]['blstats'])\n",
        "            batch_next_map_states.append(transitions[3][i]['glyphs'])\n",
        "            batch_next_stats_states.append(transitions[3][i]['blstats'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # transitions are NOT stored as [(state,action,reward,next_state,done), ()...]\n",
        "        # we have them as batches[(states_col),(actions_col),(reward_col),(next_state_col),(done_col)]\n",
        "\n",
        "        # batch_states = torch.tensor(transitions[0], dtype=torch.float).cuda()\n",
        "        batch_actions = torch.tensor(transitions[1], dtype=torch.long).cuda()\n",
        "        batch_rewards = torch.tensor(transitions[2], dtype=torch.float).cuda()\n",
        "        # batch_next_states = torch.tensor(transitions[3], dtype=torch.float).cuda()\n",
        "        batch_done = torch.tensor(transitions[4], dtype=torch.float).cuda()\n",
        "\n",
        "        batch_map_states = torch.tensor(batch_map_states, dtype=torch.float).cuda()\n",
        "        # batch_map_states = batch_map_states.unsqueeze(0)\n",
        "        batch_map_states = batch_map_states.unsqueeze(1)\n",
        "\n",
        "        batch_stats_states = torch.tensor(batch_stats_states, dtype=torch.float).cuda()\n",
        "\n",
        "        batch_next_map_states = torch.tensor(batch_next_map_states, dtype=torch.float).cuda()\n",
        "        batch_next_map_states = batch_next_map_states.unsqueeze(1)\n",
        "        batch_next_stats_states = torch.tensor(batch_next_stats_states, dtype=torch.float).cuda()\n",
        "\n",
        "        # Compute Q(s_t, a) \n",
        "        # the model computes Q(s_t), \n",
        "        # then we select the columns of actions taken. \n",
        "        # These are the actions which would've been taken\n",
        "        # for each batch state according to policy_net\n",
        "\n",
        "        # batch_actions indices the Q value - array of Q values taken for each action\n",
        "        q_values = self.model(batch_map_states, batch_stats_states).cuda()\n",
        "        q_values = q_values.gather(1, batch_actions.unsqueeze(1)).squeeze(1)\n",
        "        #with torch.no_grad():***\n",
        "        if (self.use_double_dqn == False):\n",
        "            #from the target model get the best Q values\n",
        "            next_v_values = self.target_model(batch_next_states).cuda()\n",
        "            next_v_values = next_v_values.max(1)[0]\n",
        "        else:\n",
        "            #from the model, get the index for the best Q values\n",
        "            next_v_values = self.model(batch_next_map_states, batch_next_stats_states).cuda()\n",
        "            #from target network obtain all the Q values\n",
        "            next_q_state_values = self.target_model(batch_next_map_states, batch_next_stats_states).cuda()\n",
        "            #from target network select the q values using the indices of the max next_v_values from the model network\n",
        "            next_v_values = next_q_state_values.gather(1, next_v_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
        "        \n",
        "        # Compute the expected Q values\n",
        "        #1-batch done ensures that there is a zero at the q-values where it is a final state\n",
        "        expected_q_values = (self.gamma * next_v_values * (1 - batch_done)) + batch_rewards\n",
        "\n",
        "        # Compute Huber loss\n",
        "        # expected_Q - Q-values current estimate\n",
        "        loss = torch.nn.functional.smooth_l1_loss(q_values, expected_q_values.detach())\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimiser.zero_grad()\n",
        "        # backpropagation of loss and stores gradient for updating step\n",
        "        loss.backward()\n",
        "        for param in self.model.parameters():\n",
        "            # clamped to avoid explosion/large values in learning\n",
        "            # typically used with MSE, not Huber loss\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimiser.step()\n",
        "\n",
        "\n",
        "        # rnd_loss = torch.nn.functional.mse_loss(self.predictor_network(batch_map_states, batch_stats_states), self.predictor_network(batch_map_states, batch_stats_states))\n",
        "        return loss.item()\n",
        "\n",
        "    \n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        # Copies parameters (weights and biases) and buffers from model\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def act(self, observation):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: return selected action\n",
        "        map = observation[\"glyphs\"]\n",
        "        map = torch.from_numpy(map).float().to(device)\n",
        "        map = map.unsqueeze(0)\n",
        "        map = map.unsqueeze(1)\n",
        "        stats = observation[\"blstats\"]\n",
        "\n",
        "        stats = torch.from_numpy(stats).float().to(device)\n",
        "        stats = stats.unsqueeze(0)\n",
        "        \n",
        "        \n",
        "\n",
        "        # impacts the autograd engine and deactivate it. \n",
        "        # It will reduce memory usage and speed up computations \n",
        "        # but you won’t be able to backprop (no need to whilst selecting action)\n",
        "        with torch.no_grad():\n",
        "            # state sent to CUDA to evaluate in policy_network\n",
        "            # view reshaped the matrix\n",
        "            # nn.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was found,\n",
        "            # so we pick action with the larger expected reward.\n",
        "            \n",
        "            #[1] takes the action item from the 2 items listed\n",
        "            # action = self.model(map, stats).max(0)#[1]\n",
        "            # print(\"ACTION\", action)\n",
        "            action = self.model(map, stats).max(1)[1]\n",
        "            # print(action)\n",
        "            return action.item() #gives the value\n",
        "\n",
        "class REINFORCE_agent():\n",
        "    def __init__(self, observation_space, action_space, lr=1e-4, gamma=1):\n",
        "        self.learning_rate = lr\n",
        "        self.gamma = gamma\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "        stats_observation_space = self.observation_space[\"blstats\"]\n",
        "        map_observation_space = self.observation_space[\"glyphs\"]\n",
        "        # TODO Initialise your agent's models\n",
        "        self.policy_model = Policy(map_observation_space,stats_observation_space,self.action_space)\n",
        "        self.optimiser = torch.optim.Adam(self.policy_model.parameters(), lr=self.learning_rate)\n",
        "        self.log_probs = []\n",
        "\n",
        "        # for example, if your agent had a Pytorch model it must be load here\n",
        "        # model.load_state_dict(torch.load( 'path_to_network_model_file', map_location=torch.device(device)))\n",
        "\n",
        "        # raise NotImplementedError\n",
        "\n",
        "    def act(self, observation):\n",
        "        # Perform processing to observation\n",
        "\n",
        "        # TODO: return selected action\n",
        "        map = observation[\"glyphs\"]\n",
        "        stats = observation[\"blstats\"]\n",
        "\n",
        "        x,y = stats[0], stats[1]\n",
        "\n",
        "        n = 3\n",
        "        s = 2\n",
        "        padded_map = np.pad(map,n,'edge')\n",
        "        aleph = padded_map[y-s+n:y+s+n+1,x-s+n:x+s+n+1].flatten()\n",
        "        aleph = torch.from_numpy(aleph).float().to(device)\n",
        "        aleph = aleph.unsqueeze(0)\n",
        "\n",
        "        map = torch.from_numpy(map).float().to(device)\n",
        "        map = map.unsqueeze(0)\n",
        "        map = map.unsqueeze(1)\n",
        "        stats = observation[\"blstats\"]\n",
        "        stats = torch.from_numpy(stats).float().to(device)\n",
        "        stats= stats.unsqueeze(0)\n",
        "        probs = self.policy_model.forward(map, stats, aleph)\n",
        "\n",
        "        # choose actions according to probability\n",
        "        \n",
        "        p = probs.cpu().detach().numpy().flatten()\n",
        "        action = np.random.choice(env.action_space.n, p=p)\n",
        "        self.log_probs.append(torch.log(probs[action]))\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "    def update(self,rewards):\n",
        "        self.optimiser.zero_grad()\n",
        "        returns = self.compute_returns_naive_baseline(rewards)\n",
        "\n",
        "        policy_gradient = []\n",
        "        for i in range(len(returns)):\n",
        "            # why is this negative?\n",
        "            policy_gradient.append(-self.log_probs[i] * returns[i])\n",
        "        policy_gradient = torch.stack(policy_gradient).sum()\n",
        "        loss = policy_gradient.item()\n",
        "        # back propagation\n",
        "        policy_gradient.backward()\n",
        "        self.optimiser.step()\n",
        "        self.log_probs = []\n",
        "        return loss\n",
        "\n",
        "    def compute_returns(self,rewards):\n",
        "        returns = []\n",
        "        for t in range(len(rewards)): \n",
        "            G = 0\n",
        "            for k in range(t,len(rewards)):\n",
        "                G = G + rewards[k] * self.gamma**(k-t-1)\n",
        "            returns.append(G)\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def compute_returns_naive_baseline(self,rewards):\n",
        "        baseline = np.mean(rewards)\n",
        "        std = np.std(rewards)\n",
        "\n",
        "        returns = []\n",
        "        for t in range(len(rewards)):\n",
        "            G = 0\n",
        "            for k in range(t,len(rewards)):\n",
        "                # G = G + rewards[k] * gamma**(k-t-1)\n",
        "                G = G + rewards[k]/(std+0.01) * self.gamma**(k-t-1) \n",
        "                # maybe try normalizing with returns\n",
        "            \n",
        "            G = G - baseline\n",
        "            \n",
        "            returns.append(G)\n",
        "        return returns\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, observation_space_map: spaces.Box, observation_space_stats: spaces.Box, action_space: spaces.Discrete):\n",
        "        super().__init__()\n",
        "        self.input_shape = observation_space_map.shape\n",
        "        self.n_actions = action_space.n\n",
        "\n",
        "    \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(torch.from_numpy(np.array([1])).to(device), 32, kernel_size=4, stride=3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
        "            nn.ReLU()\n",
        "        ).to(device)\n",
        "\n",
        "        # conv_out_size = self._get_conv_out(self.input_shape)\n",
        "        \n",
        "        # print(\"shape\", self.input_shape)\n",
        "        convw = self.conv2d_size_out(self.conv2d_size_out(self.conv2d_size_out(self.input_shape[0],4,3),3,2),2,1)\n",
        "        convh = self.conv2d_size_out(self.conv2d_size_out(self.conv2d_size_out(self.input_shape[1],4,3),3,2),2,1) \n",
        "        conv_out_size = convw*convh*64\n",
        "\n",
        "        stats_size = observation_space_stats.shape[0]\n",
        "        # print(\"convw\", self.conv2d_size_out(self.input_shape[0],8,4), \"convh\", convh)\n",
        "        # print(\"conv_out_size\", conv_out_size, \"stats_size\", stats_size)\n",
        "\n",
        "        # conv_out_size = self._get_conv_out((1,1,21,79))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size + stats_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.n_actions),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Softmax()\n",
        "        ).to(device)\n",
        "        \n",
        "        self.fc_aleph = nn.Sequential(\n",
        "            nn.Linear(conv_out_size + stats_size + 25, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Linear(256, self.n_actions),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Softmax()\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, x1, x2, x3=None):\n",
        "        conv_out = self.conv(x1).to(device).view(x1.size()[0], -1)\n",
        "        # print(conv_out.shape, x2.shape)\n",
        "        if type(x3) is type(None):\n",
        "            conv_and_stats = torch.cat((conv_out,x2),1)\n",
        "            return self.fc(conv_and_stats)\n",
        "        else:\n",
        "            # print(conv_out.shape, x2.shape, x3.shape)\n",
        "            conv_and_stats_and_aleph = torch.cat((conv_out,x2,x3),1)\n",
        "            return self.fc_aleph(conv_and_stats_and_aleph).flatten()\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(*shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def conv2d_size_out(self, size, kernel_size, stride):\n",
        "        return ((size - kernel_size) // stride)  + 1\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, observation_space_map: spaces.Box, observation_space_stats: spaces.Box, action_space: spaces.Discrete):\n",
        "        super().__init__()\n",
        "        self.input_shape = observation_space_map.shape\n",
        "        self.n_actions = action_space.n\n",
        "\n",
        "    \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(torch.from_numpy(np.array([1])).to(device), 32, kernel_size=4, stride=3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=2, stride=1),\n",
        "            nn.ReLU()\n",
        "        ).to(device)\n",
        "\n",
        "        # conv_out_size = self._get_conv_out(self.input_shape)\n",
        "        \n",
        "        # print(\"shape\", self.input_shape)\n",
        "        convw = self.conv2d_size_out(self.conv2d_size_out(self.conv2d_size_out(self.input_shape[0],4,3),3,2),2,1)\n",
        "        convh = self.conv2d_size_out(self.conv2d_size_out(self.conv2d_size_out(self.input_shape[1],4,3),3,2),2,1) \n",
        "        conv_out_size = convw*convh*64\n",
        "\n",
        "        stats_size = observation_space_stats.shape[0]\n",
        "        # print(\"convw\", self.conv2d_size_out(self.input_shape[0],8,4), \"convh\", convh)\n",
        "        # print(\"conv_out_size\", conv_out_size, \"stats_size\", stats_size)\n",
        "\n",
        "        # conv_out_size = self._get_conv_out((1,1,21,79))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size + stats_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.n_actions)\n",
        "        ).to(device)\n",
        "        \n",
        "        \n",
        "    def forward(self, x1, x2, x3=None):\n",
        "        conv_out = self.conv(x1).to(device).view(x1.size()[0], -1)\n",
        "        # print(conv_out.shape, x2.shape)\n",
        "        conv_and_stats = torch.cat((conv_out,x2),1)\n",
        "        return self.fc(conv_and_stats)\n",
        "\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(*shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def conv2d_size_out(self, size, kernel_size, stride):\n",
        "        return ((size - kernel_size) // stride)  + 1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfUY2FKt_Wd"
      },
      "source": [
        "# TRAINING\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJoIphHppyva"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import nle\n",
        "import random\n",
        "\n",
        "\n",
        "def run_episode_REINFORCE(env,agent=None):\n",
        "    # create instance of MyAgent\n",
        "    if agent is None:\n",
        "        print(\"no agent supplied, creating a new one\")\n",
        "        agent = REINFORCE_agent(env.observation_space, env.action_space,0.001)\n",
        "\n",
        "\n",
        "    done = False\n",
        "    episode_return = 0.0\n",
        "    state = env.reset()\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    while not done:\n",
        "        # pass state to agent and let agent decide action\n",
        "        # print(\"taking action\")\n",
        "        action = agent.act(state)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        episode_return += reward\n",
        "        state = new_state\n",
        "\n",
        "        # print(\"reward\", reward)\n",
        "    loss = agent.update(rewards)\n",
        "    print(\"episode_return\", episode_return)\n",
        "    print(\"episode_loss\", loss)\n",
        "    return episode_return, loss\n",
        "\n",
        "def train_dqn(env, num_steps):\n",
        "    # create instance of MyAgent\n",
        "    replay_buffer = ReplayBuffer(5000)\n",
        "    \n",
        "    agent = DQNAgent(env.observation_space, env.action_space,replay_buffer,True,5e-5,256,0.999)\n",
        "    \n",
        "    epsilon = 0.1\n",
        "    \n",
        "    done = False\n",
        "    episode_return = 0.0\n",
        "    state = env.reset()\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    losses = []\n",
        "    time_steps = []\n",
        "\n",
        "    for t in range(num_steps):\n",
        "        # pass state to agent and let agent decide action\n",
        "        # print(\"taking action\")\n",
        "        if np.random.random() > epsilon:\n",
        "            action = agent.act(state)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        replay_buffer.add(state, action, reward, new_state, float(done))\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        episode_return += reward\n",
        "        state = new_state\n",
        "\n",
        "        if t % 10 == 0 and len(agent.replay_memory) > agent.batch_size:\n",
        "            loss = agent.optimise_td_loss()\n",
        "            losses.append(loss)\n",
        "            time_steps.append(t)\n",
        "            \n",
        "            # print(\"loss\", loss, \"step\", t)\n",
        "\n",
        "        if done:\n",
        "            print(\"episode_return\", episode_return, \"step\", t)\n",
        "            episode_return = 0\n",
        "            env.seed(np.random.randint(6))\n",
        "            state = env.reset()\n",
        "            print(\"mean loss\", np.mean(losses[-50:]))\n",
        "\n",
        "    plt.plot(time_steps,losses)\n",
        "    plt.title(\"DQN loss over time\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(rewards)\n",
        "    plt.title(\"DQN reward over time\")\n",
        "    plt.show()\n",
        "    torch.save(agent, \"agent.pkl\")\n",
        "    return agent"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrp273ARVVpu"
      },
      "source": [
        "REINFORCE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4QtqBDNVU7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f594e5-907a-4d5d-b957-b97a389393e7"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # Seed\n",
        "    seeds = [1,2,3,4,5]\n",
        "\n",
        "    # Initialise environment\n",
        "    env = gym.make(\"NetHackScore-v0\")\n",
        "\n",
        "    # Number of times each seed will be run\n",
        "    num_runs = 100\n",
        "    agent = REINFORCE_agent(env.observation_space, env.action_space,1e-6,0.99)\n",
        "    # Run a few episodes on each seed\n",
        "    rewards = []\n",
        "    losses = []\n",
        "    for i in range(num_runs):\n",
        "        \n",
        "        run_rewards = []\n",
        "        for seed in seeds:\n",
        "            env.seed(seed)\n",
        "            episode_rewards, loss = run_episode_REINFORCE(env,agent)\n",
        "            run_rewards.append(episode_rewards)\n",
        "            losses.append(loss)\n",
        "        rewards.append(np.mean(run_rewards))\n",
        "        print(\"MEAN REWARD FOR RUN\", i,\":\",np.mean(run_rewards))\n",
        "\n",
        "    # Close environment and print average reward\n",
        "    env.close()\n",
        "    print(\"Average Reward: %f\" %(np.mean(rewards[-50:])))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode_return 28.509999999999312\n",
            "episode_loss 17549.763671875\n",
            "episode_return 59.53000000000061\n",
            "episode_loss 11875.970703125\n",
            "episode_return 3.26\n",
            "episode_loss 2018.411376953125\n",
            "episode_return 33.560000000001885\n",
            "episode_loss 16063.185546875\n",
            "episode_return 3.8200000000003858\n",
            "episode_loss 3067.18310546875\n",
            "MEAN REWARD FOR RUN 0 : 25.736000000000438\n",
            "episode_return -7.409999999999812\n",
            "episode_loss -19010.181640625\n",
            "episode_return 173.20000000000596\n",
            "episode_loss 21714.9453125\n",
            "episode_return -1.5600000000000012\n",
            "episode_loss -20177.75390625\n",
            "episode_return 68.09999999999964\n",
            "episode_loss 12389.625\n",
            "episode_return 21.29000000000007\n",
            "episode_loss 16465.39453125\n",
            "MEAN REWARD FOR RUN 1 : 50.72400000000117\n",
            "episode_return -0.2899999999997353\n",
            "episode_loss 232.1126708984375\n",
            "episode_return 104.70999999999947\n",
            "episode_loss 10312.6171875\n",
            "episode_return 89.96999999999835\n",
            "episode_loss 26280.3203125\n",
            "episode_return -1.1900000000000008\n",
            "episode_loss -13776.46484375\n",
            "episode_return 1.7700000000002947\n",
            "episode_loss -1268.997802734375\n",
            "MEAN REWARD FOR RUN 2 : 38.99399999999967\n",
            "episode_return -33.290000000001946\n",
            "episode_loss -701501.375\n",
            "episode_return 41.93000000000317\n",
            "episode_loss 11375.212890625\n",
            "episode_return 60.34000000000067\n",
            "episode_loss 9624.03515625\n",
            "episode_return -1.0900000000000007\n",
            "episode_loss -11192.4169921875\n",
            "episode_return -5.409999999999776\n",
            "episode_loss -16427.07421875\n",
            "MEAN REWARD FOR RUN 3 : 12.496000000000423\n",
            "episode_return 56.91000000000027\n",
            "episode_loss 18058.298828125\n",
            "episode_return 57.15000000000028\n",
            "episode_loss 17321.7109375\n",
            "episode_return -1.5599999999999878\n",
            "episode_loss -1983.881591796875\n",
            "episode_return -0.7700000000000005\n",
            "episode_loss -6562.091796875\n",
            "episode_return -17.219999999999853\n",
            "episode_loss -51855.765625\n",
            "MEAN REWARD FOR RUN 4 : 18.90200000000014\n",
            "episode_return 39.90000000000055\n",
            "episode_loss 12526.28515625\n",
            "episode_return -10.579999999999778\n",
            "episode_loss -31672.482421875\n",
            "episode_return 1.630000000000084\n",
            "episode_loss -2227.22314453125\n",
            "episode_return 59.400000000000965\n",
            "episode_loss 22639.1484375\n",
            "episode_return -1.0799999999998933\n",
            "episode_loss -690.1926879882812\n",
            "MEAN REWARD FOR RUN 5 : 17.854000000000386\n",
            "episode_return -8.879999999999855\n",
            "episode_loss -176564.578125\n",
            "episode_return 49.13000000000003\n",
            "episode_loss 2221.9833984375\n",
            "episode_return 76.1099999999972\n",
            "episode_loss 22926.89453125\n",
            "episode_return 22.859999999999864\n",
            "episode_loss 1490.5755615234375\n",
            "episode_return 3.1900000000000075\n",
            "episode_loss 1332.044921875\n",
            "MEAN REWARD FOR RUN 6 : 28.48199999999945\n",
            "episode_return 16.659999999999364\n",
            "episode_loss 8194.630859375\n",
            "episode_return 49.31999999999986\n",
            "episode_loss 14012.50390625\n",
            "episode_return -0.05999999999997056\n",
            "episode_loss -2360.95703125\n",
            "episode_return 10.270000000000078\n",
            "episode_loss 5912.541015625\n",
            "episode_return -0.47999999999977927\n",
            "episode_loss -1219.58154296875\n",
            "MEAN REWARD FOR RUN 7 : 15.14199999999991\n",
            "episode_return 159.10000000000258\n",
            "episode_loss 28307.7734375\n",
            "episode_return -12.559999999999677\n",
            "episode_loss -47241.0703125\n",
            "episode_return 71.52999999999948\n",
            "episode_loss 6296.4833984375\n",
            "episode_return 1.4200000000000026\n",
            "episode_loss 2811.873779296875\n",
            "episode_return -7.539999999999782\n",
            "episode_loss -15977.130859375\n",
            "MEAN REWARD FOR RUN 8 : 42.39000000000052\n",
            "episode_return -5.149999999999764\n",
            "episode_loss -9497.8388671875\n",
            "episode_return -2.9399999999998774\n",
            "episode_loss -4866.80859375\n",
            "episode_return -8.359999999999866\n",
            "episode_loss -162893.921875\n",
            "episode_return 22.049999999999734\n",
            "episode_loss 1240.2762451171875\n",
            "episode_return 25.16999999999823\n",
            "episode_loss 15851.99609375\n",
            "MEAN REWARD FOR RUN 9 : 6.153999999999692\n",
            "episode_return -3.739999999999927\n",
            "episode_loss -7404.90625\n",
            "episode_return -13.249999999999613\n",
            "episode_loss -41886.74609375\n",
            "episode_return -0.2299999999996713\n",
            "episode_loss -3369.153564453125\n",
            "episode_return 31.289999999999388\n",
            "episode_loss 8267.1123046875\n",
            "episode_return -7.589999999999883\n",
            "episode_loss -146530.5625\n",
            "MEAN REWARD FOR RUN 10 : 1.296000000000059\n",
            "episode_return 9.509999999999952\n",
            "episode_loss 6671.0283203125\n",
            "episode_return 51.00000000000013\n",
            "episode_loss 9539.4384765625\n",
            "episode_return 18.36999999999971\n",
            "episode_loss 9106.462890625\n",
            "episode_return 19.939999999997134\n",
            "episode_loss 4134.9091796875\n",
            "episode_return -1.7200000000000013\n",
            "episode_loss -23514.9453125\n",
            "MEAN REWARD FOR RUN 11 : 19.419999999999384\n",
            "episode_return -15.480000000000013\n",
            "episode_loss -41603.015625\n",
            "episode_return 112.19999999999555\n",
            "episode_loss 25387.5546875\n",
            "episode_return 42.72999999999824\n",
            "episode_loss 17632.1953125\n",
            "episode_return 55.709999999999184\n",
            "episode_loss 14788.021484375\n",
            "episode_return -4.819999999999942\n",
            "episode_loss -88826.453125\n",
            "MEAN REWARD FOR RUN 12 : 38.067999999998605\n",
            "episode_return -4.43999999999995\n",
            "episode_loss -80442.6171875\n",
            "episode_return 60.880000000000166\n",
            "episode_loss 11287.6533203125\n",
            "episode_return 39.86000000000025\n",
            "episode_loss 10765.5390625\n",
            "episode_return -1.5900000000000012\n",
            "episode_loss -19364.126953125\n",
            "episode_return 54.02000000000056\n",
            "episode_loss 22942.556640625\n",
            "MEAN REWARD FOR RUN 13 : 29.7460000000002\n",
            "episode_return -20.470000000001026\n",
            "episode_loss -75563.6796875\n",
            "episode_return 15.889999999999826\n",
            "episode_loss 7829.498046875\n",
            "episode_return -0.8600000000000005\n",
            "episode_loss -7955.6650390625\n",
            "episode_return 16.549999999999493\n",
            "episode_loss 9717.994140625\n",
            "episode_return -8.109999999999513\n",
            "episode_loss -8377.8564453125\n",
            "MEAN REWARD FOR RUN 14 : 0.5999999999997563\n",
            "episode_return -9.289999999999594\n",
            "episode_loss -9375.1728515625\n",
            "episode_return -2.249999999999979\n",
            "episode_loss -3960.87451171875\n",
            "episode_return -1.1699999999998763\n",
            "episode_loss -8214.044921875\n",
            "episode_return 73.84999999999941\n",
            "episode_loss 6211.07080078125\n",
            "episode_return -16.28999999999967\n",
            "episode_loss -44250.671875\n",
            "MEAN REWARD FOR RUN 15 : 8.97000000000006\n",
            "episode_return -6.179999999999827\n",
            "episode_loss -14512.94921875\n",
            "episode_return 46.330000000000695\n",
            "episode_loss 9424.275390625\n",
            "episode_return 43.12000000000075\n",
            "episode_loss 10262.97265625\n",
            "episode_return 57.96000000000244\n",
            "episode_loss 17190.9296875\n",
            "episode_return 67.30999999999632\n",
            "episode_loss 30297.9453125\n",
            "MEAN REWARD FOR RUN 16 : 41.70800000000007\n",
            "episode_return -7.009999999999895\n",
            "episode_loss -135286.03125\n",
            "episode_return 76.13999999999383\n",
            "episode_loss 17068.90625\n",
            "episode_return 24.22999999999973\n",
            "episode_loss 8295.521484375\n",
            "episode_return 37.120000000001134\n",
            "episode_loss 5680.66259765625\n",
            "episode_return 9.790000000000372\n",
            "episode_loss 5957.248046875\n",
            "MEAN REWARD FOR RUN 17 : 28.053999999999036\n",
            "episode_return -5.41999999999978\n",
            "episode_loss -11098.826171875\n",
            "episode_return 778.0300000000016\n",
            "episode_loss 10766.59375\n",
            "episode_return 5.930000000000042\n",
            "episode_loss 1832.470947265625\n",
            "episode_return 46.61000000000074\n",
            "episode_loss 6241.3837890625\n",
            "episode_return 29.559999999998162\n",
            "episode_loss 19485.82421875\n",
            "MEAN REWARD FOR RUN 18 : 170.94200000000015\n",
            "episode_return 31.319999999999407\n",
            "episode_loss 13837.8154296875\n",
            "episode_return 50.57000000000011\n",
            "episode_loss 8406.7890625\n",
            "episode_return 26.329999999999565\n",
            "episode_loss 9534.33984375\n",
            "episode_return -1.350000000000001\n",
            "episode_loss -16941.875\n",
            "episode_return -13.449999999999758\n",
            "episode_loss -275080.53125\n",
            "MEAN REWARD FOR RUN 19 : 18.683999999999866\n",
            "episode_return -4.910000000001368\n",
            "episode_loss -4511.76904296875\n",
            "episode_return -9.249999999999693\n",
            "episode_loss -21535.5\n",
            "episode_return 2.52000000000002\n",
            "episode_loss 1403.64013671875\n",
            "episode_return -4.89999999999994\n",
            "episode_loss -90214.59375\n",
            "episode_return 73.54999999999369\n",
            "episode_loss 35384.96875\n",
            "MEAN REWARD FOR RUN 20 : 11.40199999999854\n",
            "episode_return -13.699999999999667\n",
            "episode_loss -43552.3046875\n",
            "episode_return -6.65999999999967\n",
            "episode_loss -14364.890625\n",
            "episode_return -1.340000000000001\n",
            "episode_loss -16067.099609375\n",
            "episode_return -7.569999999999834\n",
            "episode_loss -20153.8046875\n",
            "episode_return 17.239999999999398\n",
            "episode_loss 12745.2275390625\n",
            "MEAN REWARD FOR RUN 21 : -2.405999999999955\n",
            "episode_return -8.589999999999703\n",
            "episode_loss -16672.697265625\n",
            "episode_return 8.330000000000199\n",
            "episode_loss 11409.453125\n",
            "episode_return 9.99000000000011\n",
            "episode_loss 2944.125\n",
            "episode_return 33.8900000000021\n",
            "episode_loss 13846.43359375\n",
            "episode_return 16.15999999999992\n",
            "episode_loss 10423.787109375\n",
            "MEAN REWARD FOR RUN 22 : 11.956000000000525\n",
            "episode_return -4.849999999999609\n",
            "episode_loss -9600.521484375\n",
            "episode_return 21.80999999999948\n",
            "episode_loss 7907.794921875\n",
            "episode_return 48.8300000000005\n",
            "episode_loss 7366.73486328125\n",
            "episode_return 20.399999999999803\n",
            "episode_loss 8661.75\n",
            "episode_return -4.519999999999948\n",
            "episode_loss -80693.4609375\n",
            "MEAN REWARD FOR RUN 23 : 16.334000000000046\n",
            "episode_return -14.889999999999727\n",
            "episode_loss -296644.40625\n",
            "episode_return 58.00000000000076\n",
            "episode_loss 12507.556640625\n",
            "episode_return 26.099999999999405\n",
            "episode_loss 10055.998046875\n",
            "episode_return -0.789999999999678\n",
            "episode_loss -332.759521484375\n",
            "episode_return -12.669999999999595\n",
            "episode_loss -27416.328125\n",
            "MEAN REWARD FOR RUN 24 : 11.150000000000231\n",
            "episode_return -8.639999999999775\n",
            "episode_loss -22844.66796875\n",
            "episode_return 67.90999999999565\n",
            "episode_loss 20338.677734375\n",
            "episode_return 35.219999999999864\n",
            "episode_loss 14211.091796875\n",
            "episode_return 8.020000000000575\n",
            "episode_loss 8187.5791015625\n",
            "episode_return -5.489999999999757\n",
            "episode_loss -10603.921875\n",
            "MEAN REWARD FOR RUN 25 : 19.403999999999314\n",
            "episode_return -12.169999999999785\n",
            "episode_loss -242562.421875\n",
            "episode_return 616.9400000000007\n",
            "episode_loss 22056.71484375\n",
            "episode_return 14.239999999998878\n",
            "episode_loss 10020.7138671875\n",
            "episode_return -1.3500000000000012\n",
            "episode_loss -1860.148193359375\n",
            "episode_return -8.559999999999862\n",
            "episode_loss -173236.390625\n",
            "MEAN REWARD FOR RUN 26 : 121.82000000000001\n",
            "episode_return 78.86999999999732\n",
            "episode_loss 29827.033203125\n",
            "episode_return 68.49999999999983\n",
            "episode_loss 31969.06640625\n",
            "episode_return 6.86999999999917\n",
            "episode_loss 1969.853759765625\n",
            "episode_return 3.320000000000139\n",
            "episode_loss 5777.1796875\n",
            "episode_return -1.5199999999999323\n",
            "episode_loss -1905.0814208984375\n",
            "MEAN REWARD FOR RUN 27 : 31.20799999999931\n",
            "episode_return -8.959999999999853\n",
            "episode_loss -176547.96875\n",
            "episode_return 21.259999999999845\n",
            "episode_loss 3693.099609375\n",
            "episode_return 78.05999999999938\n",
            "episode_loss 7845.2666015625\n",
            "episode_return 9.260000000000275\n",
            "episode_loss 12866.1640625\n",
            "episode_return 0.9400000000000079\n",
            "episode_loss 2043.372802734375\n",
            "MEAN REWARD FOR RUN 28 : 20.11199999999993\n",
            "episode_return 77.91999999999726\n",
            "episode_loss 22734.658203125\n",
            "episode_return -3.4899999999999696\n",
            "episode_loss -59879.890625\n",
            "episode_return 18.419999999999987\n",
            "episode_loss 3618.190673828125\n",
            "episode_return 21.239999999999746\n",
            "episode_loss 4813.8134765625\n",
            "episode_return 27.21999999999895\n",
            "episode_loss 13951.310546875\n",
            "MEAN REWARD FOR RUN 29 : 28.261999999999194\n",
            "episode_return 29.23999999999931\n",
            "episode_loss 19039.32421875\n",
            "episode_return -6.47999999999975\n",
            "episode_loss -17448.634765625\n",
            "episode_return -4.929999999999906\n",
            "episode_loss -16102.2724609375\n",
            "episode_return -1.8700000000000014\n",
            "episode_loss -24750.978515625\n",
            "episode_return -24.220000000000987\n",
            "episode_loss -500018.1875\n",
            "MEAN REWARD FOR RUN 30 : -1.6520000000002675\n",
            "episode_return -18.599999999999852\n",
            "episode_loss -46547.91796875\n",
            "episode_return 153.1800000000014\n",
            "episode_loss 16421.4765625\n",
            "episode_return 46.400000000000034\n",
            "episode_loss 4290.20751953125\n",
            "episode_return 83.759999999999\n",
            "episode_loss 10601.865234375\n",
            "episode_return 16.999999999999012\n",
            "episode_loss 10235.544921875\n",
            "MEAN REWARD FOR RUN 31 : 56.34799999999992\n",
            "episode_return -6.059999999999915\n",
            "episode_loss -115569.171875\n",
            "episode_return 58.970000000000724\n",
            "episode_loss 20262.51953125\n",
            "episode_return 56.18000000000167\n",
            "episode_loss 24916.1796875\n",
            "episode_return 11.379999999999638\n",
            "episode_loss 6202.4306640625\n",
            "episode_return -4.729999999999889\n",
            "episode_loss -11042.697265625\n",
            "MEAN REWARD FOR RUN 32 : 23.148000000000444\n",
            "episode_return 5.090000000000421\n",
            "episode_loss 5499.970703125\n",
            "episode_return 183.74000000000285\n",
            "episode_loss 18251.69921875\n",
            "episode_return 87.42000000000007\n",
            "episode_loss 31555.515625\n",
            "episode_return 25.549999999999663\n",
            "episode_loss 2305.289794921875\n",
            "episode_return 24.680000000000135\n",
            "episode_loss 16515.078125\n",
            "MEAN REWARD FOR RUN 33 : 65.29600000000063\n",
            "episode_return 57.939999999999834\n",
            "episode_loss 27463.837890625\n",
            "episode_return 48.47000000000114\n",
            "episode_loss 9979.1005859375\n",
            "episode_return 136.46999999999838\n",
            "episode_loss 26874.896484375\n",
            "episode_return -0.37000000000000016\n",
            "episode_loss -1755.0313720703125\n",
            "episode_return -2.6199999999999526\n",
            "episode_loss -8176.365234375\n",
            "MEAN REWARD FOR RUN 34 : 47.97799999999988\n",
            "episode_return -0.159999999999675\n",
            "episode_loss 521.752685546875\n",
            "episode_return 107.72999999999654\n",
            "episode_loss 24176.21875\n",
            "episode_return 2.5400000000000773\n",
            "episode_loss 3404.06298828125\n",
            "episode_return 16.709999999999045\n",
            "episode_loss 5379.6728515625\n",
            "episode_return 32.09000000000164\n",
            "episode_loss 15341.548828125\n",
            "MEAN REWARD FOR RUN 35 : 31.781999999999528\n",
            "episode_return -6.919999999999897\n",
            "episode_loss -132153.625\n",
            "episode_return 132.11000000000683\n",
            "episode_loss 23494.375\n",
            "episode_return 9.21000000000004\n",
            "episode_loss 5858.4248046875\n",
            "episode_return 69.78999999999967\n",
            "episode_loss 10855.421875\n",
            "episode_return 6.440000000000032\n",
            "episode_loss 1370.9248046875\n",
            "MEAN REWARD FOR RUN 36 : 42.12600000000133\n",
            "episode_return -11.789999999999708\n",
            "episode_loss -35588.140625\n",
            "episode_return 52.670000000000144\n",
            "episode_loss 3087.30029296875\n",
            "episode_return 58.57000000000017\n",
            "episode_loss 7324.83447265625\n",
            "episode_return 22.959999999999937\n",
            "episode_loss 2133.65087890625\n",
            "episode_return 3.780000000000056\n",
            "episode_loss 4602.630859375\n",
            "MEAN REWARD FOR RUN 37 : 25.23800000000012\n",
            "episode_return -7.149999999999892\n",
            "episode_loss -136308.859375\n",
            "episode_return 118.83999999999794\n",
            "episode_loss 20782.484375\n",
            "episode_return 133.51999999999975\n",
            "episode_loss 16777.056640625\n",
            "episode_return 3.6800000000000743\n",
            "episode_loss 2087.069091796875\n",
            "episode_return 2.3900000000000095\n",
            "episode_loss 2303.0400390625\n",
            "MEAN REWARD FOR RUN 38 : 50.25599999999958\n",
            "episode_return -2.429999999999992\n",
            "episode_loss -37840.53125\n",
            "episode_return 50.52000000000035\n",
            "episode_loss 6376.4150390625\n",
            "episode_return 8.60000000000023\n",
            "episode_loss 13752.626953125\n",
            "episode_return 29.15999999999979\n",
            "episode_loss 5486.63818359375\n",
            "episode_return 0.5000000000000386\n",
            "episode_loss -3311.677734375\n",
            "MEAN REWARD FOR RUN 39 : 17.270000000000085\n",
            "episode_return -12.580000000000895\n",
            "episode_loss -13675.953125\n",
            "episode_return 48.090000000000096\n",
            "episode_loss 4395.591796875\n",
            "episode_return 9.920000000000165\n",
            "episode_loss 5665.9072265625\n",
            "episode_return 4.440000000000202\n",
            "episode_loss 4077.041259765625\n",
            "episode_return 12.150000000000182\n",
            "episode_loss 8455.82421875\n",
            "MEAN REWARD FOR RUN 40 : 12.40399999999995\n",
            "episode_return -13.079999999999766\n",
            "episode_loss -265226.65625\n",
            "episode_return 195.47000000000335\n",
            "episode_loss 11023.376953125\n",
            "episode_return 20.009999999999525\n",
            "episode_loss 6242.4296875\n",
            "episode_return 43.649999999999956\n",
            "episode_loss 12535.1181640625\n",
            "episode_return -7.219999999999683\n",
            "episode_loss -24003.55859375\n",
            "MEAN REWARD FOR RUN 41 : 47.76600000000068\n",
            "episode_return -20.160000000000352\n",
            "episode_loss -414289.9375\n",
            "episode_return 8.570000000000222\n",
            "episode_loss 5980.25634765625\n",
            "episode_return 39.329999999999735\n",
            "episode_loss 18169.50390625\n",
            "episode_return 26.609999999999946\n",
            "episode_loss 3733.3740234375\n",
            "episode_return -1.1800000000000008\n",
            "episode_loss -13226.767578125\n",
            "MEAN REWARD FOR RUN 42 : 10.633999999999912\n",
            "episode_return 75.83999999999776\n",
            "episode_loss 20782.580078125\n",
            "episode_return -11.459999999999713\n",
            "episode_loss -33973.921875\n",
            "episode_return -2.3699999999999934\n",
            "episode_loss -34926.5390625\n",
            "episode_return 21.509999999999753\n",
            "episode_loss 3485.460693359375\n",
            "episode_return -4.529999999999667\n",
            "episode_loss -9545.44140625\n",
            "MEAN REWARD FOR RUN 43 : 15.797999999999627\n",
            "episode_return 1.8000000000000584\n",
            "episode_loss 3525.94921875\n",
            "episode_return -7.539999999999868\n",
            "episode_loss -18051.98828125\n",
            "episode_return 5.6300000000000985\n",
            "episode_loss 6214.5576171875\n",
            "episode_return -3.449999999999941\n",
            "episode_loss -10525.27734375\n",
            "episode_return 28.149999999999412\n",
            "episode_loss 2578.029296875\n",
            "MEAN REWARD FOR RUN 44 : 4.917999999999952\n",
            "episode_return -13.33999999999976\n",
            "episode_loss -267477.15625\n",
            "episode_return 47.74000000000001\n",
            "episode_loss 5084.33642578125\n",
            "episode_return 48.1899999999999\n",
            "episode_loss 16612.09765625\n",
            "episode_return 45.99000000000166\n",
            "episode_loss 20369.056640625\n",
            "episode_return 8.810000000000109\n",
            "episode_loss 7918.435546875\n",
            "MEAN REWARD FOR RUN 45 : 27.478000000000385\n",
            "episode_return 36.92000000000066\n",
            "episode_loss 20316.673828125\n",
            "episode_return 87.96999999999365\n",
            "episode_loss 20897.3046875\n",
            "episode_return -1.1499999999999793\n",
            "episode_loss -2249.811279296875\n",
            "episode_return 26.560000000000002\n",
            "episode_loss 14765.6484375\n",
            "episode_return 48.79000000000128\n",
            "episode_loss 20202.26171875\n",
            "MEAN REWARD FOR RUN 46 : 39.81799999999912\n",
            "episode_return 123.85999999999832\n",
            "episode_loss 29855.98828125\n",
            "episode_return 163.28000000000264\n",
            "episode_loss 22284.6484375\n",
            "episode_return 40.179999999999694\n",
            "episode_loss 16964.85546875\n",
            "episode_return 24.33999999999996\n",
            "episode_loss 16478.8046875\n",
            "episode_return -0.3599999999999765\n",
            "episode_loss -2003.720703125\n",
            "MEAN REWARD FOR RUN 47 : 70.26000000000013\n",
            "episode_return -2.0799999999999996\n",
            "episode_loss -30089.3984375\n",
            "episode_return -2.3499999999999543\n",
            "episode_loss -11657.65234375\n",
            "episode_return 7.680000000000069\n",
            "episode_loss 7837.732421875\n",
            "episode_return 24.65999999999944\n",
            "episode_loss 12165.4384765625\n",
            "episode_return 15.629999999998386\n",
            "episode_loss 9757.96875\n",
            "MEAN REWARD FOR RUN 48 : 8.70799999999959\n",
            "episode_return 4.840000000000371\n",
            "episode_loss 5106.2998046875\n",
            "episode_return 101.79999999999984\n",
            "episode_loss 9674.380859375\n",
            "episode_return -0.9100000000000006\n",
            "episode_loss -8697.037109375\n",
            "episode_return 19.789999999999537\n",
            "episode_loss 5231.244140625\n",
            "episode_return 27.910000000000007\n",
            "episode_loss 8185.130859375\n",
            "MEAN REWARD FOR RUN 49 : 30.68599999999995\n",
            "episode_return 1.6000000000000612\n",
            "episode_loss 3547.05859375\n",
            "episode_return 101.04000000000045\n",
            "episode_loss 13853.091796875\n",
            "episode_return 16.279999999999887\n",
            "episode_loss 5303.16455078125\n",
            "episode_return 17.67999999999907\n",
            "episode_loss 15015.662109375\n",
            "episode_return -0.31999999999995926\n",
            "episode_loss 469.408935546875\n",
            "MEAN REWARD FOR RUN 50 : 27.255999999999897\n",
            "episode_return -24.010000000000954\n",
            "episode_loss -494661.59375\n",
            "episode_return 62.6900000000011\n",
            "episode_loss 13288.9140625\n",
            "episode_return 21.6399999999999\n",
            "episode_loss 4445.32568359375\n",
            "episode_return -1.0300000000000007\n",
            "episode_loss -11116.22265625\n",
            "episode_return -11.829999999999721\n",
            "episode_loss -28587.443359375\n",
            "MEAN REWARD FOR RUN 51 : 9.492000000000065\n",
            "episode_return 4.799999999999855\n",
            "episode_loss 3472.157470703125\n",
            "episode_return 53.510000000001156\n",
            "episode_loss 9756.396484375\n",
            "episode_return 68.88999999999665\n",
            "episode_loss 17952.41015625\n",
            "episode_return 42.54000000000398\n",
            "episode_loss 9807.4931640625\n",
            "episode_return -4.9199999999999395\n",
            "episode_loss -90917.375\n",
            "MEAN REWARD FOR RUN 52 : 32.96400000000034\n",
            "episode_return -18.190000000000417\n",
            "episode_loss -65777.375\n",
            "episode_return 2.7300000000003024\n",
            "episode_loss 3142.9248046875\n",
            "episode_return 52.9500000000001\n",
            "episode_loss 2463.87109375\n",
            "episode_return 81.38999999999719\n",
            "episode_loss 16168.84765625\n",
            "episode_return 17.899999999998013\n",
            "episode_loss 10754.951171875\n",
            "MEAN REWARD FOR RUN 53 : 27.35599999999904\n",
            "episode_return -0.07999999999997433\n",
            "episode_loss 890.9337158203125\n",
            "episode_return 18.140000000000107\n",
            "episode_loss 11333.2958984375\n",
            "episode_return 3.270000000000005\n",
            "episode_loss 1542.94873046875\n",
            "episode_return 32.40999999999998\n",
            "episode_loss 12093.41796875\n",
            "episode_return 50.27000000000051\n",
            "episode_loss 15291.376953125\n",
            "MEAN REWARD FOR RUN 54 : 20.802000000000127\n",
            "episode_return -6.769999999999641\n",
            "episode_loss -7304.5224609375\n",
            "episode_return 103.76999999999461\n",
            "episode_loss 22710.7421875\n",
            "episode_return 24.07999999999958\n",
            "episode_loss 7246.4951171875\n",
            "episode_return 10.509999999999213\n",
            "episode_loss 7250.7841796875\n",
            "episode_return 28.629999999999757\n",
            "episode_loss 13569.8642578125\n",
            "MEAN REWARD FOR RUN 55 : 32.04399999999871\n",
            "episode_return 6.139999999999924\n",
            "episode_loss 5632.56787109375\n",
            "episode_return 79.15999999999883\n",
            "episode_loss 14907.4599609375\n",
            "episode_return 21.489999999999934\n",
            "episode_loss 5723.25439453125\n",
            "episode_return 22.15999999999984\n",
            "episode_loss 3302.388427734375\n",
            "episode_return 3.540000000000175\n",
            "episode_loss 667.7427368164062\n",
            "MEAN REWARD FOR RUN 56 : 26.49799999999974\n",
            "episode_return 13.530000000000431\n",
            "episode_loss 8985.6376953125\n",
            "episode_return 11.230000000000292\n",
            "episode_loss 9956.404296875\n",
            "episode_return -0.5600000000000003\n",
            "episode_loss -3715.6181640625\n",
            "episode_return 116.25999999999515\n",
            "episode_loss 33167.74609375\n",
            "episode_return 56.84000000000163\n",
            "episode_loss 22616.73828125\n",
            "MEAN REWARD FOR RUN 57 : 39.459999999999496\n",
            "episode_return 20.159999999997073\n",
            "episode_loss 14587.4658203125\n",
            "episode_return 144.20999999999805\n",
            "episode_loss 12820.783203125\n",
            "episode_return 19.64999999999985\n",
            "episode_loss 7899.6875\n",
            "episode_return 30.140000000002587\n",
            "episode_loss 9361.359375\n",
            "episode_return -0.4899999999999735\n",
            "episode_loss -2062.78369140625\n",
            "MEAN REWARD FOR RUN 58 : 42.73399999999952\n",
            "episode_return 22.430000000000124\n",
            "episode_loss 10642.185546875\n",
            "episode_return 156.0700000000002\n",
            "episode_loss 6944.5966796875\n",
            "episode_return 20.40999999999987\n",
            "episode_loss 8264.75390625\n",
            "episode_return 37.49999999999926\n",
            "episode_loss 6314.75048828125\n",
            "episode_return 12.179999999999927\n",
            "episode_loss 9555.267578125\n",
            "MEAN REWARD FOR RUN 59 : 49.717999999999876\n",
            "episode_return 33.77999999999981\n",
            "episode_loss 19331.48828125\n",
            "episode_return 8.650000000000045\n",
            "episode_loss 11880.208984375\n",
            "episode_return -0.4099999999999708\n",
            "episode_loss -2873.0751953125\n",
            "episode_return 5.4800000000000395\n",
            "episode_loss 4344.73486328125\n",
            "episode_return -3.919999999999676\n",
            "episode_loss -5799.71875\n",
            "MEAN REWARD FOR RUN 60 : 8.716000000000049\n",
            "episode_return 48.68000000000084\n",
            "episode_loss 22210.85546875\n",
            "episode_return 102.41999999999528\n",
            "episode_loss 27148.2421875\n",
            "episode_return 21.8199999999997\n",
            "episode_loss 2264.904541015625\n",
            "episode_return 69.89999999999881\n",
            "episode_loss 20386.3203125\n",
            "episode_return 12.27000000000012\n",
            "episode_loss 6439.41796875\n",
            "MEAN REWARD FOR RUN 61 : 51.017999999998956\n",
            "episode_return 24.120000000000267\n",
            "episode_loss 13105.427734375\n",
            "episode_return 62.75000000000042\n",
            "episode_loss 11855.3857421875\n",
            "episode_return -0.7800000000000005\n",
            "episode_loss -7007.744140625\n",
            "episode_return 104.13999999999463\n",
            "episode_loss 35739.7890625\n",
            "episode_return -3.809999999999897\n",
            "episode_loss -8651.904296875\n",
            "MEAN REWARD FOR RUN 62 : 37.28399999999908\n",
            "episode_return 55.49000000000175\n",
            "episode_loss 16983.66015625\n",
            "episode_return -14.239999999999677\n",
            "episode_loss -76670.671875\n",
            "episode_return 24.289999999999427\n",
            "episode_loss 9589.146484375\n",
            "episode_return -0.4599999999998953\n",
            "episode_loss -33.9908447265625\n",
            "episode_return -18.690000000000744\n",
            "episode_loss -69401.078125\n",
            "MEAN REWARD FOR RUN 63 : 9.278000000000173\n",
            "episode_return -3.699999999999965\n",
            "episode_loss -63476.3515625\n",
            "episode_return 44.300000000000814\n",
            "episode_loss 12551.919921875\n",
            "episode_return 2.3700000000000316\n",
            "episode_loss -347.0625305175781\n",
            "episode_return -2.3799999999999932\n",
            "episode_loss -36011.1171875\n",
            "episode_return 23.88\n",
            "episode_loss 8886.7138671875\n",
            "MEAN REWARD FOR RUN 64 : 12.894000000000176\n",
            "episode_return 13.150000000000063\n",
            "episode_loss 7702.3740234375\n",
            "episode_return 10.280000000000038\n",
            "episode_loss 6939.248046875\n",
            "episode_return 51.98000000000038\n",
            "episode_loss 12254.716796875\n",
            "episode_return 22.049999999999756\n",
            "episode_loss 1875.227294921875\n",
            "episode_return -7.5199999999995075\n",
            "episode_loss -11626.783203125\n",
            "MEAN REWARD FOR RUN 65 : 17.988000000000145\n",
            "episode_return -14.38999999999991\n",
            "episode_loss -46874.67578125\n",
            "episode_return 105.62999999999879\n",
            "episode_loss 9593.158203125\n",
            "episode_return 0.6700000000000361\n",
            "episode_loss -2261.176513671875\n",
            "episode_return -1.1200000000000008\n",
            "episode_loss -12524.6455078125\n",
            "episode_return 3.470000000000011\n",
            "episode_loss 9.551170349121094\n",
            "MEAN REWARD FOR RUN 66 : 18.851999999999784\n",
            "episode_return -5.529999999999762\n",
            "episode_loss -10389.794921875\n",
            "episode_return 132.82999999999902\n",
            "episode_loss 33779.09375\n",
            "episode_return 58.720000000000695\n",
            "episode_loss 10951.19921875\n",
            "episode_return 8.410000000000117\n",
            "episode_loss 5131.8916015625\n",
            "episode_return -15.309999999999548\n",
            "episode_loss -40252.0625\n",
            "MEAN REWARD FOR RUN 67 : 35.824000000000105\n",
            "episode_return -11.47000000000022\n",
            "episode_loss -12498.890625\n",
            "episode_return 61.24000000000025\n",
            "episode_loss 2830.590087890625\n",
            "episode_return 21.55999999999984\n",
            "episode_loss 4387.11328125\n",
            "episode_return 30.42000000000004\n",
            "episode_loss 10373.75390625\n",
            "episode_return -15.569999999999627\n",
            "episode_loss -52671.31640625\n",
            "MEAN REWARD FOR RUN 68 : 17.236000000000057\n",
            "episode_return -13.099999999999596\n",
            "episode_loss -31303.994140625\n",
            "episode_return -10.8199999999998\n",
            "episode_loss -32314.125\n",
            "episode_return 2.90000000000001\n",
            "episode_loss 1886.008056640625\n",
            "episode_return 0.5900000000000303\n",
            "episode_loss -739.37890625\n",
            "episode_return 11.910000000000334\n",
            "episode_loss 16256.916015625\n",
            "MEAN REWARD FOR RUN 69 : -1.7039999999998046\n",
            "episode_return 1.700000000000307\n",
            "episode_loss 2129.220947265625\n",
            "episode_return 114.62999999999889\n",
            "episode_loss 15790.2568359375\n",
            "episode_return 22.180000000000014\n",
            "episode_loss 10316.4453125\n",
            "episode_return 16.449999999999992\n",
            "episode_loss 11655.5830078125\n",
            "episode_return 15.619999999999875\n",
            "episode_loss 10270.8193359375\n",
            "MEAN REWARD FOR RUN 70 : 34.11599999999981\n",
            "episode_return -21.080000000000496\n",
            "episode_loss -430434.53125\n",
            "episode_return -7.639999999999797\n",
            "episode_loss -19801.560546875\n",
            "episode_return 17.67999999999871\n",
            "episode_loss 9898.1953125\n",
            "episode_return 6.620000000000031\n",
            "episode_loss 7940.205078125\n",
            "episode_return 42.23000000000012\n",
            "episode_loss 21970.71875\n",
            "MEAN REWARD FOR RUN 71 : 7.561999999999713\n",
            "episode_return 7.299999999999843\n",
            "episode_loss 5898.6015625\n",
            "episode_return -12.509999999999629\n",
            "episode_loss -38035.7109375\n",
            "episode_return 114.34999999999472\n",
            "episode_loss 33764.8828125\n",
            "episode_return -0.7900000000000005\n",
            "episode_loss -6998.6611328125\n",
            "episode_return 2.0900000000000074\n",
            "episode_loss 2727.716796875\n",
            "MEAN REWARD FOR RUN 72 : 22.08799999999899\n",
            "episode_return 56.90000000000024\n",
            "episode_loss 24041.55078125\n",
            "episode_return 131.4199999999994\n",
            "episode_loss 15266.595703125\n",
            "episode_return 33.55000000000002\n",
            "episode_loss 6969.248046875\n",
            "episode_return 40.73999999999991\n",
            "episode_loss 8465.59375\n",
            "episode_return 1.220000000000042\n",
            "episode_loss -3063.8203125\n",
            "MEAN REWARD FOR RUN 73 : 52.76599999999992\n",
            "episode_return -32.40000000000204\n",
            "episode_loss -142483.625\n",
            "episode_return 0.7500000000003417\n",
            "episode_loss 942.6837158203125\n",
            "episode_return 6.5200000000001115\n",
            "episode_loss 8599.3154296875\n",
            "episode_return 7.39000000000004\n",
            "episode_loss 7887.3525390625\n",
            "episode_return 27.19999999999977\n",
            "episode_loss 14768.1875\n",
            "MEAN REWARD FOR RUN 74 : 1.8919999999996442\n",
            "episode_return 17.15999999999991\n",
            "episode_loss 11079.4765625\n",
            "episode_return -10.309999999999741\n",
            "episode_loss -28023.31640625\n",
            "episode_return -0.26999999999995816\n",
            "episode_loss 933.2413330078125\n",
            "episode_return 26.00999999999841\n",
            "episode_loss 17774.83984375\n",
            "episode_return 3.2000000000004896\n",
            "episode_loss 3252.92236328125\n",
            "MEAN REWARD FOR RUN 75 : 7.157999999999822\n",
            "episode_return -13.869999999999749\n",
            "episode_loss -278748.875\n",
            "episode_return 12.150000000000206\n",
            "episode_loss 8828.314453125\n",
            "episode_return 5.3500000000000085\n",
            "episode_loss 5969.9599609375\n",
            "episode_return 28.450000000000305\n",
            "episode_loss 15085.8134765625\n",
            "episode_return 15.879999999999788\n",
            "episode_loss 6511.1708984375\n",
            "MEAN REWARD FOR RUN 76 : 9.592000000000112\n",
            "episode_return -11.4299999999998\n",
            "episode_loss -227935.40625\n",
            "episode_return 62.16999999999982\n",
            "episode_loss 12193.078125\n",
            "episode_return 111.17999999999948\n",
            "episode_loss 17775.197265625\n",
            "episode_return 97.25999999999632\n",
            "episode_loss 21844.486328125\n",
            "episode_return 27.299999999999443\n",
            "episode_loss 14765.056640625\n",
            "MEAN REWARD FOR RUN 77 : 57.29599999999906\n",
            "episode_return 23.42000000000027\n",
            "episode_loss 15064.3896484375\n",
            "episode_return 120.5999999999973\n",
            "episode_loss 16686.63671875\n",
            "episode_return 9.370000000000054\n",
            "episode_loss 2716.937744140625\n",
            "episode_return 20.299999999998807\n",
            "episode_loss -1378.3521728515625\n",
            "episode_return 54.67000000000339\n",
            "episode_loss 23837.078125\n",
            "MEAN REWARD FOR RUN 78 : 45.67199999999997\n",
            "episode_return 5.570000000000016\n",
            "episode_loss 5070.47802734375\n",
            "episode_return 39.44000000000127\n",
            "episode_loss 9215.3984375\n",
            "episode_return 3.6200000000000054\n",
            "episode_loss 456.56024169921875\n",
            "episode_return 2.4400000000000146\n",
            "episode_loss 2184.026611328125\n",
            "episode_return 8.750000000000359\n",
            "episode_loss 8406.7099609375\n",
            "MEAN REWARD FOR RUN 79 : 11.96400000000033\n",
            "episode_return 3.5000000000004974\n",
            "episode_loss 4118.20947265625\n",
            "episode_return -9.929999999999744\n",
            "episode_loss -25593.921875\n",
            "episode_return 52.43999999999971\n",
            "episode_loss 21310.87890625\n",
            "episode_return 26.139999999999844\n",
            "episode_loss 3648.5546875\n",
            "episode_return 12.210000000000262\n",
            "episode_loss 10477.455078125\n",
            "MEAN REWARD FOR RUN 80 : 16.872000000000117\n",
            "episode_return 57.740000000001835\n",
            "episode_loss 16340.2861328125\n",
            "episode_return -15.289999999999605\n",
            "episode_loss -50765.88671875\n",
            "episode_return 37.65000000000051\n",
            "episode_loss 18991.296875\n",
            "episode_return 0.43000000000000305\n",
            "episode_loss 1240.9794921875\n",
            "episode_return -23.07000000000072\n",
            "episode_loss -89229.0078125\n",
            "MEAN REWARD FOR RUN 81 : 11.492000000000408\n",
            "episode_return 21.949999999999147\n",
            "episode_loss 10644.521484375\n",
            "episode_return 106.16000000000017\n",
            "episode_loss 11104.25390625\n",
            "episode_return 20.509999999999195\n",
            "episode_loss 9871.150390625\n",
            "episode_return 47.070000000000114\n",
            "episode_loss 11143.68359375\n",
            "episode_return -0.7100000000000004\n",
            "episode_loss -5648.24267578125\n",
            "MEAN REWARD FOR RUN 82 : 38.99599999999972\n",
            "episode_return -26.420000000001245\n",
            "episode_loss -107455.875\n",
            "episode_return 8.750000000000124\n",
            "episode_loss 6466.470703125\n",
            "episode_return 10.600000000000106\n",
            "episode_loss 8717.2392578125\n",
            "episode_return 25.75999999999908\n",
            "episode_loss 3507.427734375\n",
            "episode_return 34.540000000000695\n",
            "episode_loss 13296.158203125\n",
            "MEAN REWARD FOR RUN 83 : 10.645999999999752\n",
            "episode_return -13.979999999999746\n",
            "episode_loss -283516.0625\n",
            "episode_return -3.209999999999887\n",
            "episode_loss -13426.185546875\n",
            "episode_return 6.560000000000027\n",
            "episode_loss 2595.5244140625\n",
            "episode_return 2.6600000000000152\n",
            "episode_loss 1605.89404296875\n",
            "episode_return 1.2100000000000666\n",
            "episode_loss 2940.51220703125\n",
            "MEAN REWARD FOR RUN 84 : -1.3519999999999053\n",
            "episode_return -21.900000000000624\n",
            "episode_loss -85229.8359375\n",
            "episode_return -4.269999999999953\n",
            "episode_loss -77912.875\n",
            "episode_return 26.05999999999881\n",
            "episode_loss 18931.69140625\n",
            "episode_return 24.559999999999498\n",
            "episode_loss 2008.4808349609375\n",
            "episode_return 46.959999999999205\n",
            "episode_loss 21137.85546875\n",
            "MEAN REWARD FOR RUN 85 : 14.281999999999385\n",
            "episode_return 4.780000000000344\n",
            "episode_loss 5155.4912109375\n",
            "episode_return 180.92000000001218\n",
            "episode_loss 20940.0546875\n",
            "episode_return 7.4100000000000446\n",
            "episode_loss 6532.13671875\n",
            "episode_return -0.9700000000000021\n",
            "episode_loss -976.4381103515625\n",
            "episode_return -6.929999999999616\n",
            "episode_loss -16644.66796875\n",
            "MEAN REWARD FOR RUN 86 : 37.042000000002595\n",
            "episode_return 11.389999999999084\n",
            "episode_loss 8348.509765625\n",
            "episode_return 59.99000000000112\n",
            "episode_loss 12598.70703125\n",
            "episode_return 20.819999999999784\n",
            "episode_loss 5455.6630859375\n",
            "episode_return 104.11999999999713\n",
            "episode_loss 28890.3046875\n",
            "episode_return 15.119999999999512\n",
            "episode_loss 7940.3876953125\n",
            "MEAN REWARD FOR RUN 87 : 42.28799999999932\n",
            "episode_return -7.809999999999878\n",
            "episode_loss -151267.015625\n",
            "episode_return 62.1500000000006\n",
            "episode_loss 16471.75\n",
            "episode_return 170.61999999999685\n",
            "episode_loss 34898.66796875\n",
            "episode_return 64.85999999999298\n",
            "episode_loss 18921.43359375\n",
            "episode_return -3.0499999999999665\n",
            "episode_loss -5708.6259765625\n",
            "MEAN REWARD FOR RUN 88 : 57.35399999999812\n",
            "episode_return 31.96999999999741\n",
            "episode_loss 18243.3125\n",
            "episode_return 1.870000000000158\n",
            "episode_loss 1681.2598876953125\n",
            "episode_return 48.75999999999834\n",
            "episode_loss 25475.0859375\n",
            "episode_return 5.560000000000022\n",
            "episode_loss 4570.58984375\n",
            "episode_return 1.5000000000002465\n",
            "episode_loss 2320.27587890625\n",
            "MEAN REWARD FOR RUN 89 : 17.93199999999923\n",
            "episode_return -4.099999999999957\n",
            "episode_loss -73215.7265625\n",
            "episode_return 6.170000000000078\n",
            "episode_loss 6800.14111328125\n",
            "episode_return 19.16999999999997\n",
            "episode_loss 1999.3411865234375\n",
            "episode_return 11.819999999997743\n",
            "episode_loss 10769.55078125\n",
            "episode_return 12.389999999997563\n",
            "episode_loss 8086.17919921875\n",
            "MEAN REWARD FOR RUN 90 : 9.08999999999908\n",
            "episode_return -2.5199999999999902\n",
            "episode_loss -40721.0234375\n",
            "episode_return 111.24999999999737\n",
            "episode_loss 22059.1328125\n",
            "episode_return 24.999999999998185\n",
            "episode_loss 12499.53125\n",
            "episode_return 2.9800000000000066\n",
            "episode_loss 2084.40185546875\n",
            "episode_return -7.439999999999886\n",
            "episode_loss -147165.78125\n",
            "MEAN REWARD FOR RUN 91 : 25.85399999999914\n",
            "episode_return -9.039999999999512\n",
            "episode_loss -10053.33203125\n",
            "episode_return 22.78000000000022\n",
            "episode_loss 14787.09375\n",
            "episode_return 64.75000000000016\n",
            "episode_loss 10004.4970703125\n",
            "episode_return 3.1500000000000044\n",
            "episode_loss 1929.10986328125\n",
            "episode_return -11.089999999999515\n",
            "episode_loss -36499.35546875\n",
            "MEAN REWARD FOR RUN 92 : 14.11000000000027\n",
            "episode_return 16.560000000000073\n",
            "episode_loss 6780.6396484375\n",
            "episode_return 107.89999999999941\n",
            "episode_loss 6477.10888671875\n",
            "episode_return -0.8100000000000005\n",
            "episode_loss -7363.1630859375\n",
            "episode_return 45.11000000000026\n",
            "episode_loss 8005.45263671875\n",
            "episode_return 19.819999999998\n",
            "episode_loss 11651.01171875\n",
            "MEAN REWARD FOR RUN 93 : 37.715999999999546\n",
            "episode_return 4.030000000000097\n",
            "episode_loss 6562.1875\n",
            "episode_return 100.60999999999854\n",
            "episode_loss 9681.7568359375\n",
            "episode_return 13.300000000000026\n",
            "episode_loss 4190.1748046875\n",
            "episode_return 22.429999999999957\n",
            "episode_loss 3745.48828125\n",
            "episode_return 26.719999999999484\n",
            "episode_loss 21983.7421875\n",
            "MEAN REWARD FOR RUN 94 : 33.41799999999962\n",
            "episode_return -10.549999999999677\n",
            "episode_loss -24384.703125\n",
            "episode_return 225.230000000003\n",
            "episode_loss 8434.01953125\n",
            "episode_return 50.97000000000026\n",
            "episode_loss 12020.8994140625\n",
            "episode_return 40.770000000002796\n",
            "episode_loss 15295.71875\n",
            "episode_return -4.919999999999852\n",
            "episode_loss -11569.494140625\n",
            "MEAN REWARD FOR RUN 95 : 60.30000000000132\n",
            "episode_return -10.799999999999644\n",
            "episode_loss -25574.7890625\n",
            "episode_return 18.61999999999901\n",
            "episode_loss 4925.7490234375\n",
            "episode_return 30.73000000000007\n",
            "episode_loss 14263.90625\n",
            "episode_return 52.96000000000021\n",
            "episode_loss 6852.8486328125\n",
            "episode_return 20.32999999999968\n",
            "episode_loss 7689.046875\n",
            "MEAN REWARD FOR RUN 96 : 22.367999999999864\n",
            "episode_return -9.430000000000517\n",
            "episode_loss -18213.00390625\n",
            "episode_return 47.67000000000026\n",
            "episode_loss 3964.865234375\n",
            "episode_return 45.35000000000091\n",
            "episode_loss 24145.34375\n",
            "episode_return 22.729999999999844\n",
            "episode_loss 1379.5416259765625\n",
            "episode_return -3.7600000000001916\n",
            "episode_loss -3360.02783203125\n",
            "MEAN REWARD FOR RUN 97 : 20.512000000000064\n",
            "episode_return 13.159999999999963\n",
            "episode_loss 9187.439453125\n",
            "episode_return -14.35999999999971\n",
            "episode_loss -48759.6015625\n",
            "episode_return 102.45999999999894\n",
            "episode_loss 18197.15625\n",
            "episode_return -0.9900000000000007\n",
            "episode_loss -9841.990234375\n",
            "episode_return -9.71999999999973\n",
            "episode_loss -25045.939453125\n",
            "MEAN REWARD FOR RUN 98 : 18.109999999999893\n",
            "episode_return -8.66999999999986\n",
            "episode_loss -169428.34375\n",
            "episode_return 99.64999999999324\n",
            "episode_loss 10309.8662109375\n",
            "episode_return 100.46999999999898\n",
            "episode_loss 34915.5390625\n",
            "episode_return 73.77999999999734\n",
            "episode_loss 11533.28515625\n",
            "episode_return 2.6200000000004113\n",
            "episode_loss 2964.154296875\n",
            "MEAN REWARD FOR RUN 99 : 53.569999999998025\n",
            "Average Reward: 26.128320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nXUAylrqeDA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "2ed18dc9-634a-4baa-b509-7ae0d8d73e9e"
      },
      "source": [
        "# plt.plot(losses)\n",
        "# plt.title(\"REINFORCE loss\")\n",
        "plt.plot(rewards)\n",
        "plt.title(\"REINFORCE Reward over time\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'REINFORCE Reward over time')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13i_AM-mxj8t"
      },
      "source": [
        "# ! rm -rf /content/nle_data\n",
        "# !ls -alh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L-xVtQUzyW3"
      },
      "source": [
        "torch.save(agent.policy_model.state_dict(), '/content/REINFORCE_AGENT_policy_state_dict')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ONRXEGOnfsi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg1ezUt5VYOt"
      },
      "source": [
        "DQN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBVcTIMSMFhT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63a10006-ec89-45f7-a052-226c355ab48e"
      },
      "source": [
        "env = gym.make(\"NetHackScore-v0\")\n",
        "agent = train_dqn(env, 200000)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode_return -42.82000000000001 step 4999\n",
            "mean loss 8.632760511040688\n",
            "episode_return -43.189999999999976 step 9999\n",
            "mean loss 7.5038028025627135\n",
            "episode_return 16.769999999999456 step 10756\n",
            "mean loss 4.182525602579116\n",
            "episode_return -29.550000000001567 step 15756\n",
            "mean loss 1.1967766416072845\n",
            "episode_return -24.51000000000078 step 20260\n",
            "mean loss 6.113639298280468\n",
            "episode_return 5.140000000000037 step 20820\n",
            "mean loss 6.703134147766978\n",
            "episode_return -31.190000000002076 step 24380\n",
            "mean loss 5.302425924668787\n",
            "episode_return -42.530000000000065 step 29380\n",
            "mean loss 5.924691850012168\n",
            "episode_return 1.430000000000106 step 29978\n",
            "mean loss 0.7616360372304917\n",
            "episode_return -7.219999999999824 step 31257\n",
            "mean loss 3.963486144542694\n",
            "episode_return -35.46000000000214 step 36257\n",
            "mean loss 3.4068328380584716\n",
            "episode_return 28.87000000000167 step 39495\n",
            "mean loss 4.635061608552933\n",
            "episode_return -23.620000000001518 step 42466\n",
            "mean loss 3.431641454398632\n",
            "episode_return -47.15999999999919 step 47466\n",
            "mean loss 0.4651064601168036\n",
            "episode_return 2.1700000000000834 step 48343\n",
            "mean loss 1.2863696886971594\n",
            "episode_return -45.51999999999951 step 53343\n",
            "mean loss 1.5723179545998573\n",
            "episode_return 19.699999999999417 step 53819\n",
            "mean loss 3.4255761179327964\n",
            "episode_return -43.029999999999994 step 58819\n",
            "mean loss 1.5020180022716523\n",
            "episode_return -46.04999999999941 step 63819\n",
            "mean loss 3.626245038509369\n",
            "episode_return -12.349999999999781 step 65147\n",
            "mean loss 2.0697493620216845\n",
            "episode_return 10.680000000000176 step 70147\n",
            "mean loss 1.9239618660509585\n",
            "episode_return -44.10999999999979 step 75147\n",
            "mean loss 2.0906333550810814\n",
            "episode_return -6.599999999999835 step 76271\n",
            "mean loss 2.8207723112404346\n",
            "episode_return 5.850000000000051 step 76968\n",
            "mean loss 2.3136710643768312\n",
            "episode_return -27.730000000003592 step 81968\n",
            "mean loss 0.7651652131974697\n",
            "episode_return -46.709999999999276 step 86968\n",
            "mean loss 2.5585747742652893\n",
            "episode_return 28.169999999999334 step 88296\n",
            "mean loss 9.333561661243438\n",
            "episode_return -1.9500000000000015 step 88512\n",
            "mean loss 7.703689422607422\n",
            "episode_return -15.0899999999993 step 92669\n",
            "mean loss 11.742229161262513\n",
            "episode_return 21.309999999999047 step 93384\n",
            "mean loss 5.3398125759512185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c31f40fdd750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NetHackScore-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-e5f6f2983fa8>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(env, num_steps)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimise_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtime_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-57b080b78860>\u001b[0m in \u001b[0;36moptimise_td_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbatch_stats_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_stats_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mbatch_next_map_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_next_map_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mbatch_next_map_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_next_map_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mbatch_next_stats_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_next_stats_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8phavqjDBrBc"
      },
      "source": [
        "torch.save(agent.model.state_dict(), '/content/DQN_AGENT_policy_state_dict')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w81Kfw6s2ssG"
      },
      "source": [
        "# FINAL EVALUATION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ise96GM5rgxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66d7249-77c5-4b28-af87-f2d4df5c0be4"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import nle\n",
        "import random\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def run_episode(env,seed,episode_num):\n",
        "    # create instance of MyAgent\n",
        "    # from MyAgent import MyAgent\n",
        "    # agent = MyAgent(env.observation_space, env.action_space,filepath='/content/REINFORCE_AGENT_model')\n",
        "\n",
        "\n",
        "    done = False\n",
        "    episode_return = 0.0\n",
        "    state = env.reset()\n",
        "    stats_list = [np.zeros(25),np.zeros(25)]\n",
        "    steps = 0\n",
        "    max_depth = 0\n",
        "    while not done:\n",
        "        # pass state to agent and let agent decide action\n",
        "        action = agent.act(state)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        steps += 1\n",
        "        episode_return += reward\n",
        "        # Update list of episode stats\n",
        "        stats_list = update_stat_list(stats_list,state)\n",
        "        if stats_list[0][12] > max_depth:\n",
        "            max_depth = stats_list[0][12]\n",
        "        # Check if done\n",
        "        if done:\n",
        "            row = get_stats(stats_list[0],steps,max_depth,seed,episode_num)\n",
        "\n",
        "        state = new_state\n",
        "    return episode_return,row\n",
        "\n",
        "def get_stats(stats,steps,max_depth,seed,episode_num):\n",
        "    return [1, # end_status, 1 = episode ended correctly. This is assumed here\n",
        "        stats[9], # This is the ingame score, which may differ from the total returns\n",
        "        stats[20], # time\n",
        "        steps,stats[10], # steps in this episode\n",
        "        stats[19], # health points\n",
        "        stats[18], # experience\n",
        "        stats[13], # experience level\n",
        "        stats[21], # gold\n",
        "        \"UNK\", # name of killer. env doesn't store this, so just left as unknown\n",
        "        max_depth, # The furthest depth the agent went this episode\n",
        "        episode_num, # The number of the episode\n",
        "        seed, # The env seed\n",
        "        \"nle.57.\"+str(episode_num-1)+\".ttyrec\"] # the name of the corresponding ttyrec file in stats.zip\n",
        "\n",
        "def update_stat_list(stats_list,state):\n",
        "    # Hacky stuff to get the right stats vector\n",
        "    stats_list[0] = stats_list[1]\n",
        "    stats_list[1] = list(state['blstats'])\n",
        "    return stats_list\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Directory\n",
        "    dir = os.getcwd()\n",
        "\n",
        "    # Seed\n",
        "    seeds = [1,2,3,4,5]\n",
        "\n",
        "    # Initialise environment\n",
        "    env = gym.make(\"NetHackScore-v0\")\n",
        "\n",
        "    # Generate CSV\n",
        "    stats_list = [['end_status','score','time','steps','hp','exp','exp_lev',\n",
        "    'gold','hunger','killer_name','deepest_lev','episode','seeds','ttyrec']]\n",
        "\n",
        "    # Run one episode\n",
        "    rewards = []\n",
        "    episode_num = 1\n",
        "    for seed in seeds:\n",
        "        env.seed(seed)\n",
        "        seed_rewards = []\n",
        "        reward,row = run_episode(env,seed,episode_num)\n",
        "        stats_list.append(row)\n",
        "        episode_num += 1\n",
        "        rewards.append(reward)\n",
        "    # Close environment and print average reward\n",
        "    env.close()\n",
        "    print(\"Average Reward: %f\" %(np.mean(rewards)))\n",
        "\n",
        "    # Write to csv\n",
        "    # NOTE: Unfortunately, you will need to either rename each ttyrec file to\n",
        "    #   match stats.csv or change the entry in stats.csv to point to the\n",
        "    #   appropriate ttyrec file\n",
        "    os.chdir(dir)\n",
        "    file = open('stats.csv','w+',newline='')\n",
        "    with file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(stats_list)\n",
        "    file.close()\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Reward: 27.636000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "source": [
        "Testing for submission\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqqgxilI3cvO"
      },
      "source": [
        "class MyAgent():\n",
        "    def __init__(self, observation_space, action_space, lr=1e-4, gamma=1, filepath=\"/workspace/Weights/REINFORCE_AGENT_model\"):\n",
        "        self.learning_rate = lr\n",
        "        self.gamma = gamma\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "        stats_observation_space = self.observation_space[\"blstats\"]\n",
        "        map_observation_space = self.observation_space[\"glyphs\"]\n",
        "        # TODO Initialise your agent's models\n",
        "        #\n",
        "        self.policy_model = Policy(map_observation_space,stats_observation_space,self.action_space)\n",
        "        self.optimiser = torch.optim.Adam(self.policy_model.parameters(), lr=self.learning_rate)\n",
        "        self.log_probs = []\n",
        "\n",
        "        # for example, if your agent had a Pytorch model it must be load here\n",
        "        try:\n",
        "            self.policy_model.load_state_dict(torch.load( filepath, map_location=torch.device(device)))\n",
        "        except:\n",
        "            pass\n",
        "        # raise NotImplementedError\n",
        "\n",
        "    def act(self, observation):\n",
        "        # Perform processing to observation\n",
        "\n",
        "        # TODO: return selected action\n",
        "        map = observation[\"glyphs\"]\n",
        "        stats = observation[\"blstats\"]\n",
        "\n",
        "        x,y = stats[0], stats[1]\n",
        "\n",
        "        n = 3\n",
        "        s = 2\n",
        "        padded_map = np.pad(map,n,'edge')\n",
        "        aleph = padded_map[y-s+n:y+s+n+1,x-s+n:x+s+n+1].flatten()\n",
        "        aleph = torch.from_numpy(aleph).float().to(device)\n",
        "        aleph = aleph.unsqueeze(0)\n",
        "\n",
        "        map = torch.from_numpy(map).float().to(device)\n",
        "        map = map.unsqueeze(0)\n",
        "        map = map.unsqueeze(1)\n",
        "        stats = observation[\"blstats\"]\n",
        "        stats = torch.from_numpy(stats).float().to(device)\n",
        "        stats= stats.unsqueeze(0)\n",
        "        probs = self.policy_model.forward(map, stats, aleph)\n",
        "\n",
        "        # choose actions according to probability\n",
        "        \n",
        "        p = probs.cpu().detach().numpy().flatten()\n",
        "        action = np.random.choice(env.action_space.n, p=p)\n",
        "        self.log_probs.append(torch.log(probs[action]))\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "    def update(self,rewards):\n",
        "        self.optimiser.zero_grad()\n",
        "        returns = self.compute_returns_naive_baseline(rewards)\n",
        "\n",
        "        policy_gradient = []\n",
        "        for i in range(len(returns)):\n",
        "            # why is this negative?\n",
        "            policy_gradient.append(-self.log_probs[i] * returns[i])\n",
        "        policy_gradient = torch.stack(policy_gradient).sum()\n",
        "\n",
        "        # back propagation\n",
        "        policy_gradient.backward()\n",
        "        self.optimiser.step()\n",
        "        self.log_probs = []\n",
        "        \n",
        "\n",
        "    def compute_returns(self,rewards):\n",
        "        returns = []\n",
        "        for t in range(len(rewards)): \n",
        "            G = 0\n",
        "            for k in range(t,len(rewards)):\n",
        "                G = G + rewards[k] * self.gamma**(k-t-1)\n",
        "            returns.append(G)\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def compute_returns_naive_baseline(self,rewards):\n",
        "        baseline = np.mean(rewards)\n",
        "        std = np.std(rewards)\n",
        "\n",
        "        returns = []\n",
        "        for t in range(len(rewards)):\n",
        "            G = 0\n",
        "            for k in range(t,len(rewards)):\n",
        "                # G = G + rewards[k] * gamma**(k-t-1)\n",
        "                G = G + rewards[k]/(std+0.01) * self.gamma**(k-t-1) \n",
        "                # maybe try normalizing with returns\n",
        "            \n",
        "            G = G - baseline\n",
        "            \n",
        "            returns.append(G)\n",
        "        return returns"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}